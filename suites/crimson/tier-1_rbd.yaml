# Tier1: RBD build evaluation
#
# This test suite evaluates the build to determine the execution of identified
# regression test suites.
# This suite requires node2 to be a client node with mons, mgrs and osds
# Example config - conf/crimson/dsal_rbd_conf.yaml
# Tests which are commented out are currently failing for Crimson
---
tests:
  -
    test:
      abort-on-fail: true
      module: install_prereq.py
      name: "install ceph pre-requisites"

  -
    test:
      name: cluster deployment
      desc: Execute the cluster deployment workflow.
      module: test_cephadm.py
      polarion-id:
      config:
        verify_cluster_health: true
        steps:
          - config:
              command: bootstrap
              service: cephadm
              base_cmd_args:
                verbose: true
              args:
                allow-fqdn-hostname: true
                mon-ip: node1
                orphan-initial-daemons: true
                custom_image: true
                initial-dashboard-password: Ceph_Crims
          - config:
              args:
                attach_ip_address: true
                labels: apply-all-labels
              command: add_hosts
              service: host
          - config:
              args:
                placement:
                  label: mgr
              command: apply
              service: mgr
          - config:
              args:
                placement:
                  label: mon
              command: apply
              service: mon
          - config:
              args:
                - "ceph config set global 'enable_experimental_unrecoverable_data_corrupting_features' crimson"
              command: shell
          - config:
              args:
                - "ceph osd set-allow-crimson --yes-i-really-mean-it"
              command: shell
          - config:
              args:
                - "ceph config set mon osd_pool_default_crimson true"
              command: shell
          - config:
              args:
                - "ceph config set global osd_pool_default_pg_autoscale_mode off"
              command: shell
          - config:
              args:
                all-available-devices: true
              command: apply
              service: osd
          - config:
              args:
                - "ceph osd pool create rbd"
              command: shell
          - config:
              args:
                - "rbd pool init rbd"
              command: shell
      destroy-cluster: false
      abort-on-fail: false

  -
    test:
      abort-on-fail: true
      config:
        command: add
        copy_admin_keyring: true
        id: client.1
        install_packages:
          - ceph-common
          - rbd-nbd
          - jq
          - fio
        node: node4
      desc: "Configure the client system"
      destroy-cluster: false
      module: test_client.py
      name: "configure client"
      polarion-id: CEPH-83573758

  -
    test:
      config:
        script: rbd_groups.sh
        script_path: qa/workunits/rbd
      desc: "Executing upstream RBD CLI Groups scenarios"
      module: test_rbd.py
      name: 1_rbd_cli_groups
      polarion-id: CEPH-83574239
  -
    test:
      config:
        script: import_export.sh
        script_path: qa/workunits/rbd
      desc: "Executing upstream RBD CLI Import Export scenarios"
      module: test_rbd.py
      name: 2_rbd_cli_import_export
      polarion-id: CEPH-83574240
  -
    test:
      config:
        script: luks-encryption.sh
        script_path: qa/workunits/rbd
      desc: "Executing upstream RBD LUKS Encryption scenarios"
      module: test_rbd.py
      name: 3_rbd_luks_encryption
      polarion-id: CEPH-83574242
#  -
#    test:
#      config:
#        script: cli_migration.sh
#        script_path: qa/workunits/rbd
#      desc: "Executing upstream RBD CLI Migration scenarios"
#      module: test_rbd.py
#      name: 4_rbd_cli_migration
#      polarion-id: CEPH-83574243

  - test:
      config:
        script_path: qa/workunits/rbd
        script: test_librbd_python.sh
      desc: Executig upstream LibRBD scenarios
      module: test_rbd.py
      name: 5_librbd_python
      polarion-id: CEPH-83574524

#  - test:
#      config:
#        script_path: qa/workunits/rbd
#        script: permissions.sh
#      desc: Executig upstream RBD permissions scenarios
#      module: test_rbd.py
#      name: 6_rbd_permissions
#      polarion-id: CEPH-83574525

  - test:
      config:
        script_path: qa/workunits/rbd
        script: read-flags.sh
      desc: Executig upstream RBD Read Flag scenarios
      module: test_rbd.py
      name: 7_rbd_read_flags
      polarion-id: CEPH-83574526

  - test:
      config:
        script_path: qa/workunits/rbd
        script: journal.sh
      desc: Executig upstream RBD Journal scenarios
      module: test_rbd.py
      name: 9_journal
      polarion-id: CEPH-83574527

#  - test:
#      config:
#        script_path: qa/workunits/rbd
#        script: kernel.sh
#      desc: Executig upstream RBD Kernal scenarios
#      module: test_rbd.py
#      name: 10_rbd_kernel
#      polarion-id: CEPH-83574528
#  - test:
#      config:
#        script_path: qa/workunits/rbd
#        script: krbd_exclusive_option.sh
#      desc: Executig upstream RBD kernel exclusive scenarios
#      module: test_rbd.py
#      name: 11_rbd_krbd_exclusive
#      polarion-id: CEPH-83574531
#
#  - test:
#      module: delete_clones_with_io.py
#      name: test delete clones with io
#      polarion-id: CEPH-9225
#      Desc: Create clone of an image and delete while krbd IO is running
